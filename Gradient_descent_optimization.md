## Overview
Using gradient descent optimal configuration, I predicted the optimatal configuration assumed by a groupm of 3, 4, 6, 8 and 10 dolphins given that the cost (which is the probability of failed hunt) increases as square of the pairwise distance betwen the individuals but if the individuals get too close to each other, then the cost of uncoordinated hunt grows as inverse square of the pairwise distance betwen the individuals. 
I would argue the optimal configuration is not the optimum result. Although the cost function is well defined, derivable, and convex since the second derivative of the cost function is greater than 0, the plot for the cost function versus the number of iterations is very steep. This indicates the "learning rate" (gamma) is too high. The consequence of this is that the cost function may be converging too quickly to a suboptimal solution. Gamma determines how large of a step should be taken on each iteration in the direction opposite of the gradient (again because we are trying to minimize the cost function) [2]. To large of a step, the method may skip over the actual optimal result.

This can be corrected by adjusting gamma. By using a smaller learning rate, the model can yield a more optimal solution. However, this will significantly increase computational time and expenses by increasing the number of iterations it would take for the cost function to converge. Thus, we can slower change gamma until the cost function versus number of iterations curve is smooth exponential decay
